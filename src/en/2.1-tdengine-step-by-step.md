# Setup TDengine Cluster on Kubernetes

## Service

Service config `taosd-service.yaml` for each port we will use, here note that the `metadata.name` (setted as `"taosd"`) will be used in next step:

```yaml
{{#include ../tdengine/taosd-service.yaml }}
```

## StatefulSet

We use StatefulSet config `tdengine.yaml` for TDengine.

```yaml
{{#include ../tdengine/tdengine.yaml }}
```
## Start the cluster

```sh
kubectl apply -f taosd-service.yaml
kubectl apply -f tdengine.yaml
```

The script will create a three node TDengine cluster on k8s.

Execute `show dnodes` in taos shell:

```sh
kubectl exec -i -t tdengine-0 -- taos -s "show dnodes"
kubectl exec -i -t tdengine-1 -- taos -s "show dnodes"
kubectl exec -i -t tdengine-2 -- taos -s "show dnodes"
```

Well, the current dnodes list shows:

```sql
Welcome to the TDengine shell from Linux, Client Version:3.0.0.0
Copyright (c) 2022 by TAOS Data, Inc. All rights reserved.

taos> show dnodes
   id   |            endpoint            | vnodes | support_vnodes |   status   |       create_time       |              note              |
============================================================================================================================================
      1 | tdengine-0.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:29:49.049 |                                |
      2 | tdengine-1.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:11.895 |                                |
      3 | tdengine-2.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:33.007 |                                |
Query OK, 3 rows affected (0.004610s)
```

## Scale Up

TDengine on Kubernetes could automatically scale up with:

```sh
kubectl scale statefulsets tdengine --replicas=4
```

Check if scale-up works:

```sh
kubectl get pods -l app=tdengine 
```

Results:

```text
NAME         READY   STATUS    RESTARTS   AGE
tdengine-0   1/1     Running   0          2m9s
tdengine-1   1/1     Running   0          108s
tdengine-2   1/1     Running   0          86s
tdengine-3   1/1     Running   0          22s
```

Check TDengine dnodes:

```sh
kubectl exec -i -t tdengine-0 -- taos -s "show dnodes"
```

Results:

```sql
Welcome to the TDengine shell from Linux, Client Version:3.0.0.0
Copyright (c) 2022 by TAOS Data, Inc. All rights reserved.

taos> show dnodes
   id   |            endpoint            | vnodes | support_vnodes |   status   |       create_time       |              note              |
============================================================================================================================================
      1 | tdengine-0.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:29:49.049 |                                |
      2 | tdengine-1.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:11.895 |                                |
      3 | tdengine-2.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:33.007 |                                |
      4 | tdengine-3.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:31:36.204 |                                |
Query OK, 4 rows affected (0.009594s)
```

## Scale Down

Let's try scale down from 4 to 3.

To perform a right scale-down, we should drop the last dnode in taos shell first:

```sh
kubectl exec -i -t tdengine-0 -- taos -s "drop dnode 4"
```

Then scale down to 3.

```sh
kubectl scale statefulsets tdengine --replicas=3
```

Extra replicas pods will be terminated, and retain 3 pods.

Type `kubectl get pods -l app=tdengine` to check pods.

```text
NAME         READY   STATUS    RESTARTS   AGE
tdengine-0   1/1     Running   0          4m17s
tdengine-1   1/1     Running   0          3m56s
tdengine-2   1/1     Running   0          3m34s
```

Also need to remove the pvc(if no, scale-up will be failed next):

```sh
kubectl delete pvc taosdata-tdengine-3
```

Now your TDengine cluster is safe.

Scale up again will be ok:

```sh
kubectl scale statefulsets tdengine --replicas=3
```

`show dnodes` results:

```sql
   id   |            endpoint            | vnodes | support_vnodes |   status   |       create_time       |              note              |
============================================================================================================================================
      1 | tdengine-0.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:29:49.049 |                                |
      2 | tdengine-1.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:11.895 |                                |
      3 | tdengine-2.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:33.007 |                                |
      5 | tdengine-3.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:34:35.520 |                                |
```

### Let's do something BAD Case 1

Scale it up to 4 and then scale down to 2 directly. Deleted pods are `offline` now:

```text
Welcome to the TDengine shell from Linux, Client Version:2.1.1.0
Copyright (c) 2020 by TAOS Data, Inc. All rights reserved.

taos> show dnodes
   id   |            endpoint            | vnodes | support_vnodes |   status   |       create_time       |              note              |
============================================================================================================================================
      1 | tdengine-0.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:29:49.049 |                                |
      2 | tdengine-1.taosd.default.sv... |      0 |            256 | ready      | 2022-06-22 15:30:11.895 |                                |
      3 | tdengine-2.taosd.default.sv... |      0 |            256 | offline    | 2022-06-22 15:30:33.007 | status msg timeout             |
      5 | tdengine-3.taosd.default.sv... |      0 |            256 | offline    | 2022-06-22 15:34:35.520 | status msg timeout             ||
Query OK, 4 row(s) in set (0.004293s)
```

But we can't drop tje offline dnodes, the dnode will stuck in `dropping` mode (if you call `drop dnode 'fqdn:6030'`).

### Let's do something BAD Case 2

Note that if the remaining dnodes is less than the database `replica`, it will cause error until you scale it up again.

Create database with `replica` 3, and insert data to a table:

```sh
kubectl exec -i -t tdengine-0 -- \
  taos -s \
  "create database if not exists test replica 2;
   use test; 
   create table if not exists t1(ts timestamp, n int);
   insert into t1 values(now, 1)(now+1s, 2);"
```

Scale down to replica 1 (bad behavior):

```sh
kubectl scale statefulsets tdengine --replicas=1
```

Now in taos shell, all operations with database `test` are not valid.

So, before scale-down, please check the max value of `replica` among all databases, and be sure to do `drop dnode` step.

## Clean Up TDengine StatefulSet

To complete remove tdengine statefulset, type:

```sh
kubectl delete statefulset -l app=tdengine
kubectl delete svc -l app=tdengine
kubectl delete pvc -l app=tdengine
```
